{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "634268e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from res_pred_utils import get_smooth_egovel\n",
    "\n",
    "os.chdir(\"/root/shared/Anytime-Lidar/tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609fbff1-3a51-496c-a51c-2169a04f89f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "156788 sample,\n",
      "2686189 sample_data,\n",
      "5305850 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 60.604 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 9.3 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "from nuscenes import NuScenes\n",
    "\n",
    "dataset_version = 'v1.0-trainval'\n",
    "root_path = \"../data/nuscenes/\" + dataset_version\n",
    "nusc = NuScenes(version=dataset_version, dataroot=root_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81606eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('coords', 'features', 'resolution', 'sample_tkn')\n",
      "Number of samples in dataset after removing duplicates: 54680\n",
      "inputs ater stratifying: torch.Size([26970, 11])\n",
      "outputs ater stratifying: torch.Size([26970])\n",
      "tensor([5394, 5394, 5394, 5394, 5394])\n"
     ]
    }
   ],
   "source": [
    "# CONSTANTS\n",
    "do_classification = True\n",
    "\n",
    "with open('resolution_dataset.pkl', 'rb') as f:\n",
    "    io_dict = pickle.load(f)\n",
    "    print(io_dict['fields'])\n",
    "    io_tuples = io_dict['data']\n",
    "\n",
    "# Each tuple has: 'coords', 'features', 'resolution', 'sample_tkn'\n",
    "# remove duplicates first\n",
    "scores = [tuple(io_tpl[1][:, 6].ravel()) for io_tpl in io_tuples]\n",
    "mask = np.ones(len(scores), dtype=bool)\n",
    "scores_set = set()\n",
    "for i, scr in enumerate(scores):\n",
    "    if scr in scores_set:\n",
    "        mask[i] = False\n",
    "    else:\n",
    "        scores_set.add(scr)\n",
    "\n",
    "io_tuples = [io_tpl for m, io_tpl in zip(mask, io_tuples) if m]\n",
    "print('Number of samples in dataset after removing duplicates:', len(io_tuples))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # output labels\n",
    "    ap_scores = torch.tensor([io_tpl[2] for io_tpl in io_tuples], dtype=torch.float)\n",
    "    \n",
    "    if do_classification:\n",
    "        outp_data = torch.argmax(ap_scores, dim=1)\n",
    "    else:\n",
    "        mins = ap_scores.min(1).values.unsqueeze(-1).repeat(1,5)\n",
    "        maxs = ap_scores.max(1).values.unsqueeze(-1).repeat(1,5)\n",
    "        outp_data = (ap_scores - mins) / (maxs-mins)\n",
    "    # print('ap_scores', ap_scores.size(), ap_scores.dtype)\n",
    "    # print('outp_data', outp_data.size(), outp_data.dtype)\n",
    "\n",
    "    # Find which segments has stark AP different, prioritize using them by sorting\n",
    "    ap_scores_copy = ap_scores.clone()\n",
    "    src = torch.zeros((outp_data.size(0),1))\n",
    "    ap_scores_copy.scatter_(1, outp_data.unsqueeze(-1), src)\n",
    "    outp_data_2 = torch.argmax(ap_scores_copy, dim=1)\n",
    "    diffs = ap_scores.gather(1, outp_data.unsqueeze(-1)) - ap_scores_copy.gather(1, outp_data_2.unsqueeze(-1))\n",
    "    vals, inds = torch.sort(diffs.flatten(), descending=True)\n",
    "\n",
    "    # sort and filter zeros\n",
    "    inds = inds[vals > 0]\n",
    "    outp_data = outp_data[inds]\n",
    "    io_tuples = [io_tuples[i] for i in inds]\n",
    "\n",
    "    # input labels\n",
    "    labels = [torch.tensor(io_tpl[1], dtype=torch.int)[:, -1] for io_tpl in io_tuples]\n",
    "    label_dists = torch.stack([torch.bincount(l - 1, minlength=10) for l in labels])\n",
    "    num_labels_normalizer = 100 #max(100, label_dists.max())\n",
    "    label_dists = label_dists.float() / num_labels_normalizer\n",
    "    # print('label_dists', label_dists.size())\n",
    "    \n",
    "    sample_tokens = [io_tpl[3] for io_tpl in io_tuples]\n",
    "    egovels = torch.tensor([np.linalg.norm(get_smooth_egovel(nusc, sample_tkn)[1][:2]) \\\n",
    "               for sample_tkn in sample_tokens]).float()\n",
    "    egovels[torch.isnan(egovels)] = 0.\n",
    "    egovels /= 15.0 # normalize\n",
    "    \n",
    "    inp_data = torch.cat((label_dists, egovels.unsqueeze(-1)), dim=1)\n",
    "\n",
    "    #stratify data\n",
    "    label_counts = torch.bincount(outp_data)\n",
    "    num_samples_each_label = label_counts.min()\n",
    "\n",
    "    inps_masked, outputs_masked = [], []\n",
    "    for cur_label in range(len(label_counts)):\n",
    "        mask = (outp_data == cur_label)\n",
    "        inps_masked.append(inp_data[mask][:num_samples_each_label])\n",
    "        outputs_masked.append(outp_data[mask][:num_samples_each_label])\n",
    "    inp_data = torch.cat(inps_masked)\n",
    "    outp_data = torch.cat(outputs_masked)\n",
    "    print('inputs ater stratifying:', inp_data.size())\n",
    "    print('outputs ater stratifying:', outp_data.size())\n",
    "    print(torch.bincount(outp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06be6726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 0.9919\n",
      "Epoch [20/500], Loss: 0.7829\n",
      "Epoch [30/500], Loss: 0.6091\n",
      "Epoch [40/500], Loss: 0.5408\n",
      "Epoch [50/500], Loss: 0.4427\n",
      "Epoch [60/500], Loss: 0.3587\n",
      "Epoch [70/500], Loss: 0.3646\n",
      "Epoch [80/500], Loss: 0.3872\n",
      "Epoch [90/500], Loss: 0.3114\n",
      "Epoch [100/500], Loss: 0.2794\n",
      "Epoch [110/500], Loss: 0.2764\n",
      "Epoch [120/500], Loss: 0.3362\n",
      "Epoch [130/500], Loss: 0.3395\n",
      "Epoch [140/500], Loss: 0.2427\n",
      "Epoch [150/500], Loss: 0.2438\n",
      "Epoch [160/500], Loss: 0.1803\n",
      "Epoch [170/500], Loss: 0.2000\n",
      "Epoch [180/500], Loss: 0.2667\n",
      "Epoch [190/500], Loss: 0.1890\n",
      "Epoch [200/500], Loss: 0.2486\n",
      "Epoch [210/500], Loss: 0.1607\n",
      "Epoch [220/500], Loss: 0.1937\n",
      "Epoch [230/500], Loss: 0.2177\n",
      "Epoch [240/500], Loss: 0.2903\n",
      "Epoch [250/500], Loss: 0.1947\n",
      "Epoch [260/500], Loss: 0.1721\n",
      "Epoch [270/500], Loss: 0.1771\n",
      "Epoch [280/500], Loss: 0.1739\n",
      "Epoch [290/500], Loss: 0.1799\n",
      "Epoch [300/500], Loss: 0.1703\n",
      "Epoch [310/500], Loss: 0.1531\n",
      "Epoch [320/500], Loss: 0.1421\n",
      "Epoch [330/500], Loss: 0.1641\n",
      "Epoch [340/500], Loss: 0.1624\n",
      "Epoch [350/500], Loss: 0.1615\n",
      "Epoch [360/500], Loss: 0.1887\n",
      "Epoch [370/500], Loss: 0.1621\n",
      "Epoch [380/500], Loss: 0.1369\n",
      "Epoch [390/500], Loss: 0.1562\n",
      "Epoch [400/500], Loss: 0.1294\n",
      "Epoch [410/500], Loss: 0.1195\n",
      "Epoch [420/500], Loss: 0.1549\n",
      "Epoch [430/500], Loss: 0.1833\n",
      "Epoch [440/500], Loss: 0.2425\n",
      "Epoch [450/500], Loss: 0.1324\n",
      "Epoch [460/500], Loss: 0.1573\n",
      "Epoch [470/500], Loss: 0.1073\n",
      "Epoch [480/500], Loss: 0.1214\n",
      "Epoch [490/500], Loss: 0.0965\n",
      "Epoch [500/500], Loss: 0.2280\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(11, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 5))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN().cuda()\n",
    "model.train()\n",
    "if do_classification:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()  # Using MSE as we have continuous values\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "dataset = TensorDataset(inp_data, outp_data)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_inputs, batch_outputs in dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(batch_inputs.cuda())\n",
    "        loss = criterion(predictions, batch_outputs.cuda())\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss+=loss.item()\n",
    "    \n",
    "    # Print loss every 10 epochs\n",
    "    # scheduler.step(running_loss)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea72d72b-4bb1-4529-a096-caf5bfe1039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9700, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(inp_data.cuda())\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    print((predictions == outp_data.cuda()).sum() / outp_data.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "420979d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 11])\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#ONNX export\n",
    "model.eval()\n",
    "input_names=['objcount_and_egovel']\n",
    "print('Input shape:', inp_data[:1, :].size())\n",
    "# dynamic_axes = {\n",
    "#     \"objcount_and_egovel\": {\n",
    "#         0: \"batch\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "torch.onnx.export(\n",
    "        model,\n",
    "        inp_data[:1, :].cuda(),\n",
    "        'resolution_pred_mdl.onnx',\n",
    "        input_names=input_names,\n",
    "        output_names=[\"res_scores\"],\n",
    "#         dynamic_axes=dynamic_axes,\n",
    "        opset_version=17,\n",
    "#         custom_opsets={\"cuda_slicer\": 17},\n",
    ")\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
