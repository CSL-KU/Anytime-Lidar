{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e79a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DATASET_PERIOD\"] = \"50\"\n",
    "os.environ[\"PMODE\"] = \"pmode_0002\" # same as jetson orin\n",
    "os.environ[\"STREVAL_TRAIN\"] = \"1\"\n",
    "\n",
    "os.chdir(\"/root/shared/Anytime-Lidar/tools\")\n",
    "\n",
    "import _init_path\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "from eval_utils import eval_utils\n",
    "from pcdet.config import cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file\n",
    "from pcdet.datasets import build_dataloader\n",
    "from pcdet.models import build_network, load_data_to_gpu\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import res_pred_utils\n",
    "\n",
    "def get_dataset(cfg):\n",
    "    log_file = ('./tmp_results/log_eval_%s.txt' % datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "    logger = common_utils.create_logger(log_file, rank=0)\n",
    "    test_set, test_loader, sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG, class_names=cfg.CLASS_NAMES, batch_size=1,\n",
    "        dist=False, workers=0, logger=logger, training=False\n",
    "    )\n",
    "\n",
    "    return logger, test_set, test_loader, sampler\n",
    "\n",
    "\n",
    "def get_streaming_eval_samples(num_ds_elems : int, period_ms : int, output_times_ms : np.ndarray, pred_dicts_arr):\n",
    "    # Streaming eval\n",
    "    #Now do manual sampling base on time\n",
    "\n",
    "    #times_ns should be output_times_ms\n",
    "    sampled_objects = []\n",
    "    for i in range(num_ds_elems):\n",
    "        sample_time_ms = i*period_ms\n",
    "        aft = output_times_ms > sample_time_ms\n",
    "        if aft[0] == True:\n",
    "            sampled_objects.append(None) # Should be an empty pred dicts arr\n",
    "#             print('0', end=' ')\n",
    "        elif aft[-1] == False:\n",
    "            sampled_objects.append(pred_dicts_arr[-1])\n",
    "#             print('-1', end=' ')\n",
    "        else:\n",
    "            sample_idx = np.argmax(aft) - 1\n",
    "            sampled_objects.append(pred_dicts_arr[sample_idx])\n",
    "#             print(sample_idx, end=' ')\n",
    "#     print()\n",
    "\n",
    "    return sampled_objects\n",
    "\n",
    "def do_eval(sampled_objects, model, dump_eval_dict=True):\n",
    "    #Convert them to openpcdet format\n",
    "    dataset = model.dataset\n",
    "    \n",
    "    det_annos = []\n",
    "    num_ds_elems = len(dataset)\n",
    "    for i in range(num_ds_elems):\n",
    "        data_dict = dataset.get_metadata_dict(i)\n",
    "        for k, v in data_dict.items():\n",
    "            data_dict[k] = [v] # make it a batch dict\n",
    "        pred_dicts = sampled_objects[i]\n",
    "\n",
    "        if pred_dicts is None:\n",
    "            pred_dicts = [{\n",
    "                'pred_boxes': torch.empty((0, 9)),\n",
    "                'pred_scores': torch.empty(0),\n",
    "                'pred_labels': torch.empty(0, dtype=torch.long)\n",
    "            }]\n",
    "        data_dict['final_box_dicts'] = pred_dicts\n",
    "        det_annos += dataset.generate_prediction_dicts(\n",
    "            data_dict, data_dict['final_box_dicts'], dataset.class_names, output_path=None\n",
    "        )\n",
    "\n",
    "    #nusc_annos = {} # not needed but keep it anyway\n",
    "    result_str, result_dict = dataset.evaluation(\n",
    "        det_annos, dataset.class_names,\n",
    "        eval_metric=model.model_cfg.POST_PROCESSING.EVAL_METRIC,\n",
    "        output_path='./tmp_results',\n",
    "        boxes_in_global_coords=False,\n",
    "    )\n",
    "\n",
    "    print(result_str)\n",
    "    if dump_eval_dict:\n",
    "        eval_d = {\n",
    "        'cfg': cfg,\n",
    "        'det_annos': det_annos,\n",
    "        'annos_in_glob_coords': False,\n",
    "        'resolution': model.res_idx\n",
    "        }\n",
    "    \n",
    "        eval_d['result_str'] = result_str\n",
    "    \n",
    "        with open(f'eval_data_res{model.res_idx}.pkl', 'wb') as f:\n",
    "            pickle.dump(eval_d, f)\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514a64d-d54d-4bfd-8f58-aba7f054efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(res_pred_utils)\n",
    "\n",
    "def calc_tail_ms(cur_time_point_ms, data_period_ms):\n",
    "    return cur_time_point_ms - math.floor(cur_time_point_ms / data_period_ms) * data_period_ms\n",
    "\n",
    "def get_egovel():\n",
    "    \n",
    "\n",
    "def do_streval_and_collect_data(resolution_idx, dump_eval_dict=True, sched_period_ms=2000):\n",
    "    print('***********************')\n",
    "    print(f'***RESOLUTION INDEX {resolution_idx}**')\n",
    "    print('***********************')\n",
    "\n",
    "    cfg_file = \"./cfgs/nuscenes_models/pillar01_015_02_024_03_valor.yaml\"\n",
    "    cfg_from_yaml_file(cfg_file, cfg)\n",
    "    \n",
    "    set_cfgs = ['MODEL.METHOD', '0', 'MODEL.DEADLINE_SEC', '100.0', 'MODEL.DENSE_HEAD.NAME', 'CenterHeadInf',\n",
    "                'OPTIMIZATION.BATCH_SIZE_PER_GPU', '1']\n",
    "    cfg_from_list(set_cfgs, cfg)\n",
    "    logger, test_set, test_loader, sampler = get_dataset(cfg)\n",
    "    print(f'Loaded dataset with {len(test_set)} samples')\n",
    "    \n",
    "    ckpt_file=\"../output/nuscenes_models/pillar01_015_02_024_03_valor/default/ckpt/checkpoint_epoch_30.pth\"\n",
    "    \n",
    "    model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=test_set)\n",
    "    model.load_params_from_file(filename=ckpt_file, logger=logger, to_cpu=False)\n",
    "    # model.pre_hook_handle.remove()\n",
    "    # model.post_hook_handle.remove()\n",
    "    model.eval() # should be run with @torch.no_grad\n",
    "    model.cuda()\n",
    "\n",
    "    data_period_ms = 50\n",
    "    num_samples = len(test_set)\n",
    "    \n",
    "    cur_sample_idx = 0\n",
    "    sim_cur_time_ms = 0.\n",
    "    last_exec_time_ms = 100.\n",
    "    target_sched_time_ms = 0.\n",
    "    pred_dicts_arr = []\n",
    "    output_times_ms = []\n",
    "    processed_inds = set()\n",
    "    \n",
    "    model.calibrate()\n",
    "    do_res_sched = (resolution_idx == -1)\n",
    "    model.res_idx = 0 if do_res_sched else resolution_idx\n",
    "\n",
    "    resolution_stats = [0] * model.num_res\n",
    "    with alive_bar(num_samples, force_tty=True, max_cols=160, manual=True) as bar:\n",
    "        while cur_sample_idx < num_samples:\n",
    "            lbd = model.latest_batch_dict # save bef its modified\n",
    "            with torch.no_grad():\n",
    "                #batch_dict = model.dataset.collate_batch([data_dict])\n",
    "                #load_data_to_gpu(batch_dict)\n",
    "                pred_dicts, ret_dict = model([cur_sample_idx])\n",
    "    \n",
    "            # Predict the execution time as if the DNN were to be executed on target platform\n",
    "            batch_dict = model.latest_batch_dict\n",
    "            num_points = batch_dict['points'].size(0)\n",
    "            num_voxels = np.array([batch_dict['bb3d_num_voxels']])\n",
    "            xlen = batch_dict['x_lims'][1] - batch_dict['x_lims'][0]\n",
    "            last_exec_time_ms = model.calibrators[model.res_idx].pred_exec_time_ms(\n",
    "               num_points, num_voxels, xlen)\n",
    "\n",
    "            sim_cur_time_ms += last_exec_time_ms\n",
    "\n",
    "            if do_res_sched and sim_cur_time_ms >= target_sched_time_ms:\n",
    "                sample_tkn = batch_dict['metadata'][0]['token']\n",
    "                if lbd is not None and not batch_dict['scene_reset']:\n",
    "                    # time diff can be more than 50ms, but its ok\n",
    "                    prev_sample_tkn = lbd['metadata'][0]['token']\n",
    "                    egovel = res_pred_utils.get_2d_egovel(\n",
    "                            model.token_to_ts[prev_sample_tkn],\n",
    "                            model.token_to_pose[prev_sample_tkn],\n",
    "                            model.token_to_ts[sample_tkn],\n",
    "                            model.token_to_pose[sample_tkn])\n",
    "                    \n",
    "                else: # assume its zero\n",
    "                    egovel = np.zeros(2)\n",
    "\n",
    "                #NOTE Time prediction does not work all the time, \n",
    "                # res_exec_times_sec = []\n",
    "                # for ridx in range(model.num_res):\n",
    "                #     if ridx == model.res_idx:\n",
    "                #         res_exec_times_sec.append(last_exec_time_ms)\n",
    "                #     else:\n",
    "                #         # WARNING!!!!! num voxels and xlen change depending on resolution!\n",
    "                #         res_exec_times_sec.append(model.calibrators[ridx].pred_exec_time_ms(\n",
    "                #                 num_points, num_voxels, xlen))\n",
    "                # print(res_exec_times_sec)\n",
    "                res_exec_times_sec = np.array([0.247, 0.147, 0.107, 0.091, 0.077]) # mean\n",
    "                ratio = (last_exec_time_ms/1000.) / res_exec_times_sec[model.res_idx]\n",
    "                chosen_res = res_pred_utils.pick_best_resolution(res_exec_times_sec * ratio,\n",
    "                                                                 egovel, pred_dicts[0])\n",
    "                resolution_stats[chosen_res] += 1\n",
    "                model.res_idx = chosen_res\n",
    "                #NOTE I need to consider the sched time as well and add to sim cur time ms\n",
    "                target_sched_time_ms += sched_period_ms\n",
    "            \n",
    "            pred_dicts_arr.append(pred_dicts)\n",
    "            output_times_ms.append(sim_cur_time_ms)\n",
    "            processed_inds.add(cur_sample_idx)\n",
    "    \n",
    "            #Dynamic scheduling\n",
    "            cur_tail = calc_tail_ms(sim_cur_time_ms, data_period_ms)\n",
    "            pred_finish_time = sim_cur_time_ms + last_exec_time_ms #NOTE I can also use mean exec time\n",
    "            next_tail = calc_tail_ms(pred_finish_time, data_period_ms)\n",
    "            if next_tail < cur_tail:\n",
    "                # Sleep, extra 1 ms is added to make sure sleet time is enough\n",
    "                sim_cur_time_ms += data_period_ms - cur_tail + 1\n",
    "    \n",
    "            cur_sample_idx = int(sim_cur_time_ms / data_period_ms)\n",
    "            if cur_sample_idx in processed_inds:\n",
    "                print(f'ERROR, trying to process already processed sample {cur_sample_idx}')\n",
    "    \n",
    "            bar(cur_sample_idx / num_samples)\n",
    "    \n",
    "    output_times_ms = np.array(output_times_ms)\n",
    "    model.print_time_stats()\n",
    "    print('Resolution selection stats:')\n",
    "    print(resolution_stats)\n",
    "    \n",
    "    #We have:\n",
    "    # pred_dicts_arr\n",
    "    # output_times_ms\n",
    "    # processed_inds\n",
    "\n",
    "    os.environ[\"RESOLUTION_IDX\"] = str(model.res_idx)\n",
    "    sampled_objects = get_streaming_eval_samples(len(test_set), data_period_ms, output_times_ms, pred_dicts_arr)\n",
    "    print(f'Sampled {len(sampled_objects)} objects')\n",
    "    result_str = do_eval(sampled_objects, model, dump_eval_dict)\n",
    "    return resolution_stats, result_str\n",
    "\n",
    "# from IPython.display import clear_output\n",
    "# os.environ[\"FINE_GRAINED_EVAL\"] = \"1\"\n",
    "# for res_idx in range(5):\n",
    "#     do_streval_and_collect_data(res_idx, True)\n",
    "\n",
    "results = []\n",
    "os.environ[\"FINE_GRAINED_EVAL\"] = \"0\"\n",
    "resolution_stats, result_str = do_streval_and_collect_data(-1, False)\n",
    "results.append((resolution_stats, result_str))\n",
    "\n",
    "with open(\"output.txt\", \"w\") as f:\n",
    "    for period, resolution_stats, result_str in results:\n",
    "        f.write(f'{resolution_stats}\\n')\n",
    "        f.write(result_str)\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
